# ALPHAFUSION – Stock Prediction with Multimodal Transformers

**ALPHAFUSION** is a deep learning framework designed to predict directional stock movements by integrating historical market data with financial news sentiment. This project explores multiple neural architectures, including BiLSTM, Transformer, Attention-BiLSTM, Temporal Convolutional Networks (TCN), and a hybrid CNN–Transformer model, to capture both short-term temporal patterns and long-range dependencies in financial time series.  

The pipeline leverages transformer-based textual embeddings (e.g., FinBERT) to extract semantic and sentiment signals from news articles, which are fused with numerical market features for robust forecasting. Extensive experiments demonstrate that hybrid architectures combining convolutional feature extraction with attention-based sequence modeling provide superior predictive performance, highlighting the value of multimodal, sentiment-aware approaches in stock movement prediction.

**Key Features:**
- Domain-adapted transformer embeddings for financial news.
- Regression and classification models for price prediction and directional trends.
- Hybrid CNN–Transformer architecture for local and global temporal pattern learning.
- Modular, extensible codebase suitable for research and practical experimentation.

**Repository Link:** [ALPHAFUSION GitHub](https://github.com/Alpha-github/ALPHAFUSION)

**Project Overview**: This repository contains the code, notebooks, and utilities used for the STATS507 project (stock prediction using price timeseries + news). It includes ETL/feature-prep notebooks, FinBERT embedding pipelines, compact PyTorch models (regression & classification), and utilities for selecting top-N tickers.

**Contents**:
- **`model_training.ipynb`**: Main experimental notebook — dataset preparation, FinBERT embedding pipeline, TimeSeries/Torch datasets, model definitions (BiLSTM / Transformer / advanced classifiers), training loops, diagnostics, and per-epoch metrics logging.
- **`preprocessing.ipynb`**: Data cleaning, timezone handling, and ETL for merging price + news into machine-learning-ready tables.
- **`utils_topn.py`**: Helper functions `get_top_n_by_history` and `get_top_n_by_news` for selecting subsets of tickers.
- **`data/`**: Raw and processed data used by the notebooks (embeddings, parquet files, etc.).
- **`models/`**: Saved model state dictionaries (created when running training cells).
- **`metrics/`**: Per-epoch CSV exports and plots generated by training cells (e.g., `regression_epoch_metrics.csv`*, `adv_training_epoch_metrics.csv`).
- **`requirements.txt`**: Python dependencies used by the notebooks.

**Quick Setup (Windows / PowerShell)**
- **Create & activate a venv (recommended)**:

```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
``` 

- **Install dependencies**:

```powershell
python -m pip install --upgrade pip
pip install -r requirements.txt
```

Notes:
- The `requirements.txt` contains general packages (PyTorch, Transformers, PyTorch Lightning, pytorch-forecasting, scikit-learn, etc.). If you need GPU-enabled PyTorch, install the correct `torch`/`torchvision` build from https://pytorch.org/ before installing the other requirements.
- On Windows, DataLoader `num_workers` is set conservatively in notebooks; you may increase this on Linux or when running on a server.

**Kaggle datasets (download instructions)**

If you'd like to download the external datasets used during development (stock price histories and the massive news DB), follow these steps. These commands assume you are on Windows PowerShell and your venv is activated.

1. Install the Kaggle CLI (if not already installed):

```powershell
pip install kaggle
```

2. Place your `kaggle.json` API token in the default location (`%USERPROFILE%\.kaggle\kaggle.json`) or set the env vars `$env:KAGGLE_USERNAME` and `$env:KAGGLE_KEY` in your session.

3. Download and unzip the datasets:

```powershell
# stock price history (large)
kaggle datasets download -d jakewright/9000-tickers-of-stock-market-data-full-history -p .\data\kaggle\stock_prices --unzip
# massive news DB
kaggle datasets download -d miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests -p .\data\kaggle\stock_news --unzip
```
4. Verify the downloaded files

Notes:
- These datasets are large — ensure you have sufficient disk space before downloading.
- If you get permission errors, confirm you accepted the dataset rules on Kaggle and that `kaggle.json` is correct.
- If the `kaggle` CLI is not found after installation, make sure the active Python environment is the one where you installed `kaggle` and that the venv is activated.

**Running the Notebooks**
- Start Jupyter (Lab or Notebook):

```powershell
jupyter lab
# or
jupyter notebook
```

- Open and run `model_training.ipynb`. Run cells in order — the notebook is structured so earlier cells prepare data and define model classes used by later cells.

**What the notebooks produce**
- Models saved to: `models/` (files like `bilstm_state.pt`, `adv_classifier_best.pt`).
- Per-epoch metrics CSV files saved to: `metrics/` (CSV files and some figures).
- Embeddings and preprocessed data saved under: `data/` (parquet and embedding files).